{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e19ff-fba2-45de-97be-6427a6b09d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate.\n",
    "---------\n",
    "Linear Regression:\n",
    "    Linear Regression is a supervised regression model.\n",
    "    Equation of linear regression:\n",
    "        y = a0 + a1x1 + a2x2 + ‚Ä¶ + aixi\n",
    "        Here,\n",
    "        y = response variable\n",
    "        xi = ith predictor variable\n",
    "        ai = average effect on y as xi increases by 1\n",
    "    In Linear Regression, we predict the value by an integer number.\n",
    "    Here no activation function is used.\n",
    "    Here no threshold value is needed.\n",
    "    Here we calculate Root Mean Square Error(RMSE) to predict the next weight value.\n",
    "    Here dependent variable should be numeric and the response variable is continuous to value.\n",
    "    It is based on the least square estimation.\n",
    "    Here when we plot the training datasets, a straight line can be drawn that touches maximum plots.\n",
    "    Linear regression is used to estimate the dependent variable in case of a change in independent variables. For example, predict the price of houses.\n",
    "    Linear regression assumes the normal or gaussian distribution of the dependent variable.\n",
    "    Applications of linear regression:\n",
    "        Financial risk assessment\n",
    "        Business insights\n",
    "        Market analysis\n",
    "Logistic Regression:\n",
    "    Logistic Regression is a supervised classification model.\n",
    "    Equation of logistic regression\n",
    "        y(x) = e(a0 + a1x1 + a2x2 + ‚Ä¶ + aixi) / (1 + e(a0 + a1x1 + a2x2 + ‚Ä¶ + aixi))\n",
    "        Here,\n",
    "        y = response variable\n",
    "        xi = ith predictor variable\n",
    "        ai = average effect on y as xi increases by 1\n",
    "    In Logistic Regression, we predict the value by 1 or 0.\n",
    "    Here activation function is used to convert a linear regression equation to the logistic regression equation\n",
    "    Here a threshold value is added.\n",
    "    Here we use precision to predict the next weight value.\n",
    "    Here the dependent variable consists of only two categories. Logistic regression estimates the odds outcome of the dependent variable given a set of quantitative or categorical independent variables.\n",
    "    It is based on maximum likelihood estimation.\n",
    "    Any change in the coefficient leads to a change in both the direction and the steepness of the logistic function. It means positive slopes result in an S-shaped curve and negative slopes result in a Z-shaped curve.\n",
    "    Logistic regression is used to calculate the probability of an event. For example, classify if tissue is benign or malignant.\n",
    "    Logistic regression assumes the binomial distribution of the dependent variable.\n",
    "    Applications of logistic regression:\n",
    "        Medicine\n",
    "        Credit scoring\n",
    "        Hotel Booking\n",
    "        Gaming\n",
    "        Text editing\n",
    "        \n",
    "In case of mdeical field, logistic regression is more appropriate than linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a685f0c-d0b0-4bbe-a1cd-965d95d0bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?\n",
    "---------\n",
    "The choice of cost function, log loss or cross-entropy, is significant for logistic regression. \n",
    "It quantifies the disparity between predicted probabilities and actual outcomes, providing a measure of how well the model aligns with the ground truth.\n",
    "\n",
    "For logistic regression, the Cost\n",
    " function is defined as:\n",
    "\n",
    "Cost(‚ÑéùúÉ(ùë•),ùë¶)={‚àílog(‚ÑéùúÉ(ùë•)) if y = 1\n",
    "              ‚àílog(1‚àí‚ÑéùúÉ(ùë•)) if y = 0 }\n",
    "\n",
    "the ùúÉs in the hypothesis function:\n",
    "    ‚ÑéùúÉ(ùë•)=1/(1+ùëíùúÉ‚ä§ùë•)\n",
    "    \n",
    "To optimise the cost function we have to run the gradient descent function on each parameter:\n",
    "\n",
    "repeat until convergence \n",
    "{\n",
    "    ùúÉùëó:=ùúÉùëó‚àíùõº(‚àÇ/‚àÇùúÉùëó)ùêΩ(ùúÉ)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb76c32-389a-48c3-94e9-77e0dd9cba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "----------\n",
    "Regularization is a way of finding a good bias-variance tradeoff by tuning the complexity of the model.\n",
    "\n",
    "Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. \n",
    "The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee925bc-8d79-4753-a7cb-e66660b54178",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?\n",
    "----------\n",
    "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model at varying threshold values. \n",
    "The ROC curve is the plot of the true positive rate against the false positive rate at each threshold setting.\n",
    "\n",
    "The Area Under the ROC curve (AUC) is an aggregated metric that evaluates how well a logistic regression model classifies positive and negative outcomes at all possible cutoffs. \n",
    "It can range from 0.5 to 1, and the larger it is the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7fc6a-c5b2-4eaa-a0ed-8f2cc1f15065",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?\n",
    "-----------\n",
    "Types of Feature Selection Methods in ML:\n",
    "    Chi-square Test\n",
    "        The Chi-square test is used for categorical features in a dataset. \n",
    "        We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores. \n",
    "    Fisher‚Äôs Score\n",
    "        Fisher score is one of the most widely used supervised feature selection methods. \n",
    "        The algorithm we will use returns the ranks of the variables based on the fisher‚Äôs score in descending order. \n",
    "    Correlation Coefficient\n",
    "        Correlation is a measure of the linear relationship between 2 or more variables. \n",
    "        Through correlation, we can predict one variable from the other. \n",
    "    Variance Threshold\n",
    "        The variance threshold is a simple baseline approach to feature selection. \n",
    "        It removes all features whose variance doesn‚Äôt meet some threshold. \n",
    "        By default, it removes all zero-variance features, i.e., features with the same value in all samples.\n",
    "    Forward Feature Selection\n",
    "        This is an iterative method wherein we start with the performing features against the target features. \n",
    "    Backward Feature Elimination\n",
    "        This method works exactly opposite to the Forward Feature Selection method. \n",
    "        Here, we start with all the features available and build a model. \n",
    "        Next, we the variable from the model, which gives the best evaluation measure value.\n",
    "    Exhaustive Feature Selection\n",
    "        This is the most robust feature selection method covered so far. \n",
    "        This is a brute-force evaluation of each feature subset. \n",
    "        This means it tries every possible combination of the variables and returns the best-performing subset.\n",
    "    Recursive Feature Elimination\n",
    "        Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
    "    LASSO Regularization (L1)\n",
    "        Regularization consists of adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model, i.e., to avoid over-fitting.\n",
    "    Random Forest Importance\n",
    "        Random Forests is a kind of Bagging Algorithm that aggregates a specified number of decision trees. \n",
    "        The tree-based strategies used by random forests naturally rank by how well they improve the purity of the node, or in other words, a decrease in the impurity (Gini impurity) over all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b842a2-e918-4db2-bae1-0da278495e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?\n",
    "-----------\n",
    "We can handle imbalanced dataset by using below techniques:\n",
    "    Random under-sampling.\n",
    "    Random over-sampling.\n",
    "    Synthetic over-sampling: SMOTE.\n",
    "    Choose the algorithm wisely.\n",
    "    Play with the loss function.\n",
    "    Solve an anomaly detection problem.\n",
    "\n",
    "Some of the strategies for dealing with imbalanced datasets are:\n",
    "    1. oversampling: oversample the minority class\n",
    "    2. understampling: undersample the majority class\n",
    "    3. combination: use a combination of oversampling and undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030aecaa-0189-443d-b6f6-4ce3f26912f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?\n",
    "---------\n",
    "Some common issues or challenges faced while using Logistic Regression are:\n",
    "    If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting.\n",
    "    It can only be used to predict discrete functions. Hence, the dependent variable of Logistic Regression is bound to the discrete number set.\n",
    "    It is tough to obtain complex relationships using logistic regression. More powerful and compact algorithms such as Neural Networks can easily outperform this algorithm.\n",
    "    Logistic regression is less inclined to over-fitting but it can overfit in high dimensional datasets.One may consider Regularization (L1 and L2) techniques to avoid over-fittingin these scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
