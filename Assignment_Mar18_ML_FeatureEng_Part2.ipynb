{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb0089-52a9-430f-be13-00735387c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the Filter method in feature selection, and how does it work?\n",
    "-------\n",
    "Filter methods measure the relevance of features by their correlation with dependent variable.\n",
    "In this method, features are filtered based on general characteristics (some metric such as correlation) of the dataset such correlation with the dependent variable. \n",
    "Filter method is performed without any predictive model. \n",
    "It is faster and usually the better approach when the number of features are huge. \n",
    "Avoids overfitting but sometimes may fail to select best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cac2d-e76c-4d72-8e4b-641753630896",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
    "-------\n",
    "Filter methods perform the feature selection independently of construction of the classification model, \n",
    "while the Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab142b-5a65-49c8-a5c4-cc617131d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What are some common techniques used in Embedded feature selection methods?\n",
    "-------\n",
    "Techniques use din Embedded feature selection method are:\n",
    "Regularization – This method adds a penalty to different parameters of the machine learning model to avoid over-fitting of the model. \n",
    "    This approach of feature selection uses Lasso (L1 regularization) and Elastic nets (L1 and L2 regularization). \n",
    "    The penalty is applied over the coefficients, thus bringing down some coefficients to zero. \n",
    "    The features having zero coefficient can be removed from the dataset.\n",
    "Tree-based methods – These methods such as Random Forest, Gradient Boosting provides us feature importance as a way to select features as well. \n",
    "    Feature importance tells us which features are more important in making an impact on the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fd253-a6a4-4b16-a8bc-1af7236bac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are some drawbacks of using the Filter method for feature selection?\n",
    "------\n",
    "The common disadvantage of filter methods is that they ignore the interaction with the classifier and each feature is considered independently thus ignoring feature dependencies. \n",
    "In addition, it is not clear how to determine the threshold point for rankings to select only the required features and exclude noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80faeb5b-486e-4332-8dc8-a1d1e6be5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?\n",
    "---------\n",
    "Filter methods are much faster compared to wrapper methods as they do not involve training the models.\n",
    "Filter methods are suitable for applications that mostly deal with the storage and retrieval of high dimensional datasets. \n",
    "It is computationally efficient than wrapper methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e5add-b382-4917-a4fe-8bca929e9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n",
    "--------\n",
    "Here, we will select the feature on the basis of their scores in various statistical tests for their correlation with the outcome variable.\n",
    "We will use below techniques:\n",
    "- Pearson’s Correlation: It is used as a measure for quantifying linear dependence between two continuous variables X and Y. \n",
    "    Its value varies from -1 to +1.\n",
    "- LDA: Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.\n",
    "- ANOVA: ANOVA stands for Analysis of variance. \n",
    "    It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. \n",
    "    It provides a statistical test of whether the means of several groups are equal or not.\n",
    "- Chi-Square: It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354032b-8b68-488f-8215-a19c3d2dbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model.\n",
    "-------\n",
    "Embedded methods combine the qualities’ of filter and wrapper methods. \n",
    "It’s implemented by algorithms that have their own built-in feature selection methods.\n",
    "Following techniques can be used:\n",
    "- Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "- Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f7169-f1e1-45f9-9f97-6e3a71109ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor.\n",
    "----------\n",
    "Here, we try to use a subset of features and train a model using them. \n",
    "Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset.\n",
    "We can use following techniques:\n",
    "- Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. \n",
    "    In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "- Backward Elimination: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. \n",
    "    We repeat this until no improvement is observed on removal of features.\n",
    "- Recursive Feature elimination: It is a greedy optimization algorithm which aims to find the best performing feature subset. \n",
    "    It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. \n",
    "    It constructs the next model with the left features until all the features are exhausted.\n",
    "    It then ranks the features based on the order of their elimination."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
