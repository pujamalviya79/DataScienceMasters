{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8a8e5-279e-447c-86ac-4d96563f8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n",
    "--------\n",
    "Overfitting : Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. \n",
    "    Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. \n",
    "    The overfitted model has low bias and high variance.\n",
    "    \n",
    "    Overfitting can cause the degraded performance of the machine learning model. \n",
    "    But the main cause is overfitting, so there are some ways by which we can reduce the occurrence of overfitting in our model.\n",
    "        Cross-Validation\n",
    "        Training with more data\n",
    "        Removing features\n",
    "        Early stopping the training\n",
    "        Regularization\n",
    "        Ensembling\n",
    "    \n",
    "Underfitting : Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. \n",
    "    To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. \n",
    "    As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "\n",
    "    In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
    "    \n",
    "    Underfitting can be avoided:\n",
    "        By increasing the training time of the model.\n",
    "        By increasing the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d702a3a-9628-4b14-9259-37c9918f1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "--------\n",
    "Techniques to reduce overfitting:\n",
    "- Increase training data.\n",
    "- Reduce model complexity.\n",
    "- Early stopping during the training phase\n",
    "- Ridge Regularization and Lasso Regularization\n",
    "- Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474f218-d621-414f-b16d-72d7c2e8a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "---------\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, \n",
    "i.e., it only performs well on training data but performs poorly on testing data.\n",
    "\n",
    "Scenarios where Underfitting can happen in ML are:\n",
    "- High bias and low variance \n",
    "- The size of the training dataset used is not enough.\n",
    "- The model is too simple.\n",
    "- Training data is not cleaned and also contains noise in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba5b88-df15-40fd-a320-dc77f8b71b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "---------\n",
    "If the algorithm is too simple then it may be on high bias and low variance condition and thus is error-prone. \n",
    "If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. \n",
    "In the latter condition, the new entries will not perform well. \n",
    "There is something between both of these conditions, known as Trade-off or Bias Variance Trade-off.\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. \n",
    "An algorithm can’t be more complex and less complex at the same time.\n",
    "\n",
    "Bias and variance are inversely connected. \n",
    "\n",
    "While building the machine learning model, it is really important to take care of bias and variance in order to avoid overfitting and underfitting in the model. \n",
    "If the model is very simple with fewer parameters, it may have low variance and high bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784d817-2702-48b7-8a16-1296a4fb65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "--------\n",
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. \n",
    "Your model is underfitting the training data when the model performs poorly on the training data.\n",
    "Overfitting can be identified by checking validation metrics such as accuracy and loss.\n",
    "The best method to detect overfit models is by testing the machine learning models on more data with with comprehensive representation of possible input data values and types.\n",
    "Typically, part of the training data is used as test data to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c28ff0-6a93-4b3f-adc3-6b51c4479b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "--------\n",
    "\n",
    "Bias : \n",
    "    When an algorithm is employed in a machine learning model and it does not fit well, a phenomenon known as bias can develop. Bias arises in several situations.\n",
    "    The disparity between the values that were predicted and the values that were actually observed is referred to as bias.\n",
    "    The model is incapable of locating patterns in the dataset that it was trained on, and it produces inaccurate results for both seen and unseen data.\n",
    "Variance :\n",
    "    The term \"variance\" refers to the degree of change that may be expected in the estimation of the target function as a result of using multiple sets of training data.\n",
    "    A random variable's variance is a measure of how much it varies from the value that was predicted for it.\n",
    "    The model recognizes the majority of the dataset's patterns and can even learn from the noise or data that isn't vital to its operation.\n",
    "    \n",
    "When the accuracy of both the training and testing data are poor, or when the error of both the training and testing data are high, ‘high variance’ is how it’s referred to.\n",
    "Example: K-Nearest Neighbour (KNN) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1cffd-e1b1-4455-86ce-4d63b66dfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.\n",
    "---------\n",
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting.\n",
    "\n",
    "To avoid the occurrence of overfitting, we may use regularization.\n",
    "Regularization is a technique that adds information to a model to prevent the occurrence of overfitting.\n",
    "It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. \n",
    "In this context, the reduction of the capacity of a model involves the removal of extra weights.\n",
    "Regularization removes extra weights from the selected features and redistributes the weights evenly. \n",
    "This means that regularization discourages the learning of a model of both high complexity and flexibility. \n",
    "A highly flexible model is one that possesses the freedom to fit as many data points as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
