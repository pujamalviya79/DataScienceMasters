{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f3a00-dcba-4b8f-8c45-ec4795104199",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "-----------\n",
    "GridSearchCV is a technique for finding the optimal parameter values from a given set of parameters in a grid. \n",
    "It's essentially a cross-validation technique. \n",
    "The model as well as the parameters must be entered. \n",
    "After extracting the best parameter values, predictions are made.\n",
    "\n",
    "GridSearchCV works by defining a grid of hyperparameters and then systematically training and evaluating a machine learning model for each hyperparameter combination. \n",
    "The process of training and evaluating the model for each combination is called cross-validation. \n",
    "The best set of hyperparameters is then selected based on the performance metric.\n",
    "\n",
    "To use GridSearchCV, we need to specify the following:\n",
    "The hyperparameters to be tuned: This includes specifying a range of values for each hyperparameter.\n",
    "The machine learning model: This includes the type of model you want to use and its parameters.\n",
    "The performance metric to be used: This is the metric that will be used to evaluate the performance of the different hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47eec54-6893-432c-a7e3-e5d07e4dd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?\n",
    "-----------\n",
    "Grid Search:\n",
    "    Grid search can search a large number of hyperparameters.\n",
    "    Grid Search can become computationally expensive as the number of hyperparameters increases. \n",
    "Random search:\n",
    "    Random Search can search a larger number of hyperparameters without becoming too computationally expensive.\n",
    "    Random Search samples hyperparameters randomly.\n",
    "    \n",
    "Grid Search CV is more suitable when the search space is relatively small and computationally feasible.\n",
    "\n",
    "Random Search CV tends to be more efficient when the search space is large, as it requires evaluating only a subset of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ee8a3-fc54-466d-9818-86e5d987ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "-----------\n",
    "Data leakage is one of the main machine Learning errors and can affect the overall production performance and validation accuracy of the model to a great extent.\n",
    "\n",
    "This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production. \n",
    "In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate.\n",
    "\n",
    "A scenario when ML model already has information of test data in training data, but this information would not be available at the time of prediction, called data leakage. \n",
    "It causes high performance while training set, but perform poorly in deployment or production.\n",
    "\n",
    "Example: Data exposed in transit — Data transmitted via emails, API calls, chat rooms, and other communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00a300-9491-47dc-9ed4-598be5cb56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "-----------\n",
    "We can prevent data leakage by following ways:\n",
    "    1. Extract the appropriate set of features\n",
    "    2. Add an individual validation set.\n",
    "    3. Apply data pre-processing separately to both data sets\n",
    "    4. Time-series data\n",
    "    5. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0150fc9-6eca-47f9-b632-e028ac5e9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "-----------\n",
    "A confusion matrix is a table that is used to define the performance of a classification algorithm. \n",
    "A confusion matrix visualizes and summarizes the performance of a classification algorithm.\n",
    "\n",
    "Confusion matrix displays the number of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ea366-25f8-4ef0-9419-cd0c3767c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "-----------\n",
    "Precision of a model:\n",
    "    The number of True Positives divided by the sum of true positives and false positives.\n",
    "Recall of a model:\n",
    "    The number of True Positives divided by the sum of true positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f50a9-a0cd-4b53-b79a-72ddd2e9535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "-----------\n",
    "A confusion matrix is a table that sums up the performance of a classification model. \n",
    "It works for binary and multi-class classification.\n",
    "    The confusion matrix shows the number of correct predictions: true positives (TP) and true negatives (TN). \n",
    "    It also shows the model errors: false positives (FP) are “false alarms,” and false negatives (FN) are missed cases.\n",
    "    Using TP, TN, FP, and FN, you can calculate various classification quality metrics, such as precision and recall.\n",
    "\n",
    "To create the matrix, we need to draw a table. For binary classification, it is a 2x2 table with two rows and columns. \n",
    "Rows typically show the actual classes, and columns show the predicted classes. \n",
    "Then populate the matrix with the numbers of true and false predictions on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a70ca-28c9-4066-bfb5-5b51ed0e7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n",
    "-----------\n",
    "Some common metrics that can be derived from a confusion metrix are:\n",
    "1. Accuracy:\n",
    "    Accuracy is used to measure the performance of the model. \n",
    "    It is the ratio of Total correct instances to the total instances. \n",
    "        Accuracy = {TP+TN}/{TP+TN+FP+FN}   \n",
    "2. Precision:\n",
    "    Precision is a measure of how accurate a model’s positive predictions are. \n",
    "    It is defined as the ratio of true positive predictions to the total number of positive predictions made by the model.\n",
    "        Precision = {TP}/{TP+FP}      \n",
    "3. Recall:\n",
    "    Recall measures the effectiveness of a classification model in identifying all relevant instances from a dataset. \n",
    "    It is the ratio of the number of true positive (TP) instances to the sum of true positive and false negative (FN) instances.\n",
    "        Recall = {TP}/{TP+FN}     \n",
    "4. F1-Score:\n",
    "    F1-score is used to evaluate the overall performance of a classification model. \n",
    "    It is the harmonic mean of precision and recall,\n",
    "        F1-Score = {2*Precision*Recall}/{Precision + Recall}               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b07f2-c21a-42ba-90be-784a6abb297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "-----------\n",
    "Accuracy is calculated as Total number of true correct predictions (TP+TN) divided by the total number of a dataset (P+N).\n",
    "In other words we can say that, Accuracy is a metrix that measures how often a machine learning model correctly predicts the outcome.\n",
    "    Accuracy = Correct Predictions/All Predictions\n",
    "    Accuracy = {TP+TN}/{TP+TN+FP+FN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0abf51b-4a97-4b7e-84f7-28498ad28d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?\n",
    "----------\n",
    "Confusion matrices are the key to identifying data imbalances or model bias. \n",
    "In an ideal scenario, the test data have an approximately even number of labels, and the predictions made by the model also are approximately spread across the labels. \n",
    "For 1000 samples, a model that is unbiased, but often gets answers wrong as TP=250, TN=250, FP=250, FN=250.\n",
    "We can tell that the input data is unbiased, because the row sums are the same (500 each), indicating that half the labels are “true”, and half are “false”. \n",
    "Similarly, we can see that the model is giving unbiased responses because it is returning true half the time and false the other half of the time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
