{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fee9f-449d-45c1-8aec-8d0f21139a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "-------\n",
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. \n",
    "In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "\n",
    "To calculate R2 you need to find the sum of the residuals squared and the total sum of squares. \n",
    "Start off by finding the residuals, which is the distance from regression line to each data point. \n",
    "Work out the predicted y value by plugging in the corresponding x value into the regression line equation.\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "score=r2_score(Y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0ce88-ccc9-4d8c-81d7-aff6c32ffc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "--------\n",
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. \n",
    "The adjusted R-squared increases when the new term improves the model more than would be expected by chance. \n",
    "It decreases when a predictor improves the model by less than expected.\n",
    "\n",
    "R2 represents the proportion of the variance in the dependent variable explained by the independent variables. \n",
    "Adjusted R-squared considers the number of predictors in the model and penalizes excessive variables, providing a more accurate measure of the model's goodness of fit, especially with multiple predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab3e9c-4530-4556-ab42-3a2e28997c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "--------\n",
    "It is better to use Adjusted R-squared when there are multiple variables in the regression model. \n",
    "This would allow us to compare models with differing numbers of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405c0c9-edd6-4120-b4b6-e9ff9945e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "---------\n",
    "RMSE: \n",
    "    Root Mean Squared Error is the square root of Mean Squared error. \n",
    "    It measures the standard deviation of residuals.\n",
    "    RMSE=Sqrt(MSE)\n",
    "MSE:\n",
    "    Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. \n",
    "    It measures the variance of the residuals.\n",
    "    MAE = 1/N (summation of i=1-n)(predicted value - mean value)^2\n",
    "MAE: \n",
    "    The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. \n",
    "    It measures the average of the residuals in the dataset.\n",
    "    MAE = 1/N (summation of i=1-n)(predicted value - mean value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab146f-4388-4739-a715-ba4a899c3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "--------\n",
    "RMSE:\n",
    "    Advantages:\n",
    "        Has the same unit as the original data, making it easier to interpret.\n",
    "    Disadvantages:\n",
    "        Sensetive to outliers.\n",
    "MSE:\n",
    "    Advantages:\n",
    "        Penalizes larger erros more heavily, giving it more sensitivity to outliers.\n",
    "    Disadvantages:\n",
    "        Harder to interpret than MAE, as it is not in the same unit as the original data.\n",
    "MAE:\n",
    "    Advantages:\n",
    "        Easy to interpret and understand. Less sensitive to outliers.\n",
    "    Disadvantages:\n",
    "        Does not take into account the direction of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd459e11-4ff1-4e3a-a10e-7d72d58507d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "--------\n",
    "The LASSO method regularizes model parameters by shrinking the regression coefficients, reducing some of them to zero. \n",
    "The feature selection phase occurs after the shrinkage, where every non-zero value is selected to be used in the model.\n",
    "\n",
    "While lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\n",
    "\n",
    "Suppose we have a high dimensionality and high correlation in our dataset, then prefer L1(lasso) regularisation since it penalises less important features more and makes them zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a198d1-fa9b-491c-90e9-6f96058013a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "--------\n",
    "Regularization is a technique that penalizes the coefficient. \n",
    "In an overfit model, the coefficients are generally inflated. \n",
    "Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. \n",
    "The coefficients are added to the cost function of the linear equation.\n",
    "\n",
    "Regularization works by adding a penalty or complexity term to the complex model. \n",
    "Let's consider the simple linear regression equation:\n",
    "y= β0+β1x1+β2x2+β3x3+⋯+βnxn +b\n",
    "In the above equation, Y represents the value to be predicted\n",
    "\n",
    "X1, X2, …Xn are the features for Y.\n",
    "\n",
    "β0,β1,…..βn are the weights or magnitude attached to the features, respectively. \n",
    "Here represents the bias of the model, and b represents the intercept.\n",
    "\n",
    "Linear regression models try to optimize the β0 and b to minimize the cost function.  \n",
    "Now, we will add a loss function and optimize parameter to make the model that can predict the accurate value of Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de685f1-0499-4aef-838a-2a54b17d8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "-------\n",
    "It presumes a linear relationship between the input and output variables, linear regression is unable to accurately fit complicated datasets. \n",
    "Since the relationships between the dataset's variables are rarely linear in real-world situations, a straight line cannot accurately represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ed729-c7ce-4471-be3e-e4547b1a6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "--------\n",
    "Comparing the two models: - \n",
    "    Model A has a lower RMSE (10), indicating that it might perform better when large errors are of particular concern. \n",
    "    Model B has a lower MAE (8), suggesting that it might perform better in terms of overall average prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fadbb-b839-4500-8f22-1bafff9938a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "--------\n",
    "If we have many features with high correlation and you need to take away the useless features then LASSO is the better solution. \n",
    "If the number of features greater than the number of observations and many features with multi-collinearity, Ridge regularization is a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c2b39c-db66-4fc9-88cb-e8dabbbc0277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b139bce-5521-4801-b6ac-bf3bc36a9d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
